信息增益


信息熵是度量样本集合纯度最常用的一种指标.假定当前样本集合D中第k类样本所占比重为$p_k(k=1,\dots,|\mathcal{y}|)$,则D的信息熵定义为
$$
Ent(D)=-\sum_{k=1}^{|y|}p_k\log_2p_k
$$ 
Ent(D)的值越小，则D的纯度越高.

假定离散属性a有V个可能的取值$\{a^1,a^2,\dots,a^V\}$,若使用a来对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$.根据上式计算出$D^v$的信息熵，在考虑到不同的分支节点所包含的样本数不同，给分支节点赋予权重$|D^v|/|D|$,即样本数越多的分支节点的影响越大，于是可计算出用属性a对样本集D进行划分所获得的信息增益
$$
Gain(D,a)=Ent(D)-\sum_{v=1}^V\dfrac{|D^v|}{|D|}Ent(D^v)
$$

一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的的纯度提升越大.因此，可用信息增益来进行决策树的划分属性选择。ID3决策树学习就是以信息增益为准则来进行划分属性。

信息增益比
信息增益比其实就是对信息增益除以了一个$a$的熵：  

$$
\frac{Gain(D,a)}{Gain(a)}
$$
C4.5算法以信息增益比为准则来进行划分属性

CART决策树使用基尼指数来选择划分属性,数据集D的纯度可用基尼值来度量:
$$
\begin{aligned}
	Gini(D)&=\sum_{k=1}^{|\mathcal{Y}|}\sum_{k^{'}\neq k}p_kp_{k^{'}}\\
	&=1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2
\end{aligned}
$$
Gini(D)反映了从数据集D中随机抽取两个样本，其类别不一致的概率.因此,Gini(D)越小，则数据集D的纯度越高。


属性a的基尼指数定义为
$$
	Gini\_index(D,a)=\sum_{k=1}^{V}\dfrac{|D^v|}{|D|}Gini(D^v)
$$
在候选属性集合A中选择使划分后基尼指数最小的属性作为最优划分属性
决策树划分基本流程

1.选择最优属性

2.根据属性不同取值划分数据

3.在划分后的数据再重复前两步，直到跳出循环